{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4801c2a",
   "metadata": {},
   "source": [
    "# ps1010 mnist 영상인식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7cb47",
   "metadata": {},
   "source": [
    "## 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675c7f3",
   "metadata": {},
   "source": [
    "- BK0287-0101\n",
    "- MNIST 실습은 0부터 9까지의 숫자를 인식하는 실습이다.\n",
    "- 각 샘플은 28x28 픽셀을 가지고 있다.\n",
    "- 파이토치는 Dataset 모듈에서 MNIST 데이터셋을 읽어온다.\n",
    "- 이 실습에서는 파이토치를 이용해서 다중 클래스 분류 모형을 훈련시키고 훈련된 모델이 테스트 샘플에서 얼마나 잘 작동하는지를 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30978358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e60cc",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af0fa2",
   "metadata": {},
   "source": [
    "- 이 모델은 합성곱 계층, 드롭아웃계층, 선형/완전 연결 계층으로 구성\n",
    "- 모든 계층은 torch.nn 모듈을 통해서 사용\n",
    "-  __init__ 함수는 모델의 중추 아키텍처, 즉 각 계층의 뉴런 개수와 함께 모든 계층을 정의\n",
    "- forward 함수는 신경망에서 정보를 앞으로 전달 따라서 이 함수는 각 계층에서 쓸 수 있는 모든 활성화 함수와 계층 뒤에 올 폴링이나 드롭아웃 계층 포함\n",
    "- forward 함수는 마지막 계층의 출력, 즉 모델의 예측을 반환하며, 이 출력은 타깃 출력(정답)과 동일한 차원을 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f5f2b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1) # 입력 1채널(흑백이미지라서), 출력 16채널, 커널크기 3(일반적으로 홀수), 스트라이드 1\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1) #출력 채널수를 32로 한것은 더 많은 특징 추출을 위해서\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8cb962e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.310609\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 1.924133\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.313336\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 0.796470\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 0.819801\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.678430\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.477235\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.526729\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 0.469570\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.243354\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.535540\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.250727\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.449397\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.415974\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.327603\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.501861\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.146283\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.372229\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.093487\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.173131\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.296608\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.086363\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.334908\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.037923\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.368311\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.237165\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 0.357118\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.204113\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.050533\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.347196\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.222008\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.196307\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.219461\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.371997\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.076640\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.063637\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.239423\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.055383\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.360913\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.023938\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.091675\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.103321\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.190044\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.072944\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.060187\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.073172\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.164674\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.372170\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.078066\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.129855\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.092128\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.164644\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.046490\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.270464\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 0.052892\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.031950\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.038460\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.019325\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.053178\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.208981\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.077057\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.056482\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.073632\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.118282\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.053564\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.226998\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.257602\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.246156\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.094773\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.073710\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.141951\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.288561\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.080773\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.124614\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.228794\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.145711\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.069470\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.144412\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.027573\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.028125\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.577887\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 0.449275\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.129446\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.031079\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.155822\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.013211\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.050209\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.129425\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.036181\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.394779\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.117680\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.209140\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.011749\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.054243\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 0.511668\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.148937\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.253182\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.080742\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.080144\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.024576\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.009845\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.124462\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.091768\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.060347\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.254630\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.187278\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.014570\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.036776\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.004277\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.159036\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.247975\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.165650\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.020769\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.171206\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.095169\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.146039\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.054088\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.192695\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.040132\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.115668\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.052174\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 0.456471\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.049946\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.156750\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.024614\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.117552\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.071479\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.227584\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.183102\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.206875\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.161943\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.061622\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.010551\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.049372\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.080375\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.046545\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.237970\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.041292\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.135201\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.067930\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.275516\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.019263\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.171098\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.004647\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.216872\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.058772\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.101013\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.065428\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.015367\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 0.186005\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.195530\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.203482\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.013121\n",
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.094128\n",
      "epoch: 1 [49280/60000 (82%)]\t training loss: 0.197300\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.301263\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.125687\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.061686\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.004499\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.004558\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.249185\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.025598\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.012293\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.012412\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.014274\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.039375\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.016903\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.035044\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.024567\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.005434\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.157171\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.010556\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.001412\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.050422\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.071518\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.043187\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.158121\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.061694\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.042585\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.077480\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.003835\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.071643\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.036640\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.006439\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.098434\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.009693\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.029291\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.028656\n",
      "\n",
      "Test dataset: Overall Loss: 0.0479, Overall Accuracy: 9845/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.075698\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.055071\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.160146\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.095397\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.047407\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.002443\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.063052\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.032045\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.133718\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.014536\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.047828\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.012530\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.097237\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.025002\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.024353\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.032080\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.186925\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.166297\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.098077\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.044796\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.032175\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.062097\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.020596\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.064281\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.024767\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.199381\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.045193\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.015518\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.020066\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.003326\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.037821\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.005519\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.009228\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.002546\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.079078\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.016833\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.012243\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.120761\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.053045\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.061277\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.008337\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.184124\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.034670\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.086315\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.053228\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.025730\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.010458\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.002853\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.097315\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.042974\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.105631\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.164642\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.005448\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.373247\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.038784\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.052734\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.092705\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.096151\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.001856\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.065804\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.159037\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.008316\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.106004\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.016675\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.122087\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.014653\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.078983\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.038760\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.014895\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.004829\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.001739\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.007576\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.165336\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.113387\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.110958\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.022384\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.020373\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.086011\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.047534\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.024318\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.116360\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.018324\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.001088\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.006212\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.059867\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.445822\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.113997\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 0.003168\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 0.036717\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.095503\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.008521\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.001779\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 0.029157\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.070354\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.017105\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.027431\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.027088\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.156768\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.062776\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.009541\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.005300\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.005720\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.015025\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.001287\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.139033\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.010272\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.011606\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.322803\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.004529\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.072160\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.020083\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.027895\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.003890\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.052780\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.090674\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.002149\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.497427\n",
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.005962\n",
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.089726\n",
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.018642\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.023832\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.081781\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.026557\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.006496\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.145347\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.013661\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.224560\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.504058\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.056767\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.073024\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.169390\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.005694\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.009082\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.030739\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.006198\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.002554\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.044424\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.228694\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.158914\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.019425\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.003482\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.051014\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.044056\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.051398\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.070854\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.023761\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.130592\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.044132\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.031991\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.031451\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.107672\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.143929\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.037954\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.007448\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.003075\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.004475\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.099167\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.038001\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.003061\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.005109\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.014250\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.173459\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.002985\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.003505\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.099276\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.035642\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.003578\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.126000\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.175342\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.002460\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.426825\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.090889\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.021490\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.075617\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.011608\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.021301\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.000758\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.065469\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.016555\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.027795\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.074182\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.172041\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.064053\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.003793\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.005715\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.017967\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.005941\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.008535\n",
      "\n",
      "Test dataset: Overall Loss: 0.0413, Overall Accuracy: 9865/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 7\n",
      "Ground truth is : 7\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))\n",
    "            \n",
    "            \n",
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))\n",
    "    \n",
    "    \n",
    "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('psdata/ps1010', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
    "    batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('psdata/ps1010', train=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,)) \n",
    "                   ])),\n",
    "    batch_size=500, shuffle=False)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)\n",
    "\n",
    "\n",
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)\n",
    "    \n",
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
    "print(f\"Ground truth is : {sample_targets[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf2fbb6",
   "metadata": {},
   "source": [
    "# ps1015 MNIST 다른 샘플"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce9745",
   "metadata": {},
   "source": [
    "- 링크 : https://gaussian37.github.io/dl-pytorch-conv2d/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b05b0b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "batch_size = 1\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        root=\"psdata/ps1015\",  # 현재 경로에 datasets/MNIST/ 를 생성 후 데이터를 저장한다.\n",
    "        train=True,  # train 용도의 data 셋을 저장한다.\n",
    "        download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),  # tensor 타입으로 데이터 변경\n",
    "            # data를 normalize 하기 위한 mean과 std 입력\n",
    "            transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "        ])\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a9bba5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# data set의 shape를 확인한다\n",
    "image, label = next(iter(train_loader))\n",
    "print(image.shape, label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bd6716d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conv2d 사용테스트\n",
    "nn.Conv2d(\n",
    "    in_channels= 1,\n",
    "    out_channels= 3,\n",
    "    kernel_size = 5,\n",
    "    stride = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55b11c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 3, kernel_size=(5, 5), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "# 위의 conv2d를 이용하여 layer를 생성한다.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "layer = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=5, stride=1).to(device)\n",
    "print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cdad9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.0410,  0.0524, -0.0451, -0.1452,  0.1425],\n",
      "          [ 0.1289,  0.0498, -0.0810, -0.1295,  0.0828],\n",
      "          [-0.0299,  0.1864,  0.0017,  0.1545, -0.1141],\n",
      "          [ 0.1186,  0.0774,  0.0989, -0.1613,  0.1759],\n",
      "          [ 0.0505,  0.0448, -0.1707, -0.1054,  0.0503]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0966, -0.0924,  0.1285, -0.1245,  0.0437],\n",
      "          [ 0.0249,  0.0396,  0.0139,  0.1116, -0.1172],\n",
      "          [ 0.1209,  0.0552,  0.0991,  0.0571, -0.1287],\n",
      "          [ 0.0951, -0.1193, -0.0153, -0.1852,  0.1637],\n",
      "          [ 0.0478,  0.1413, -0.1903,  0.1355,  0.0565]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1703,  0.1487,  0.1113,  0.0168, -0.0505],\n",
      "          [ 0.1785,  0.1056, -0.0446, -0.1551,  0.1421],\n",
      "          [-0.1537,  0.1825, -0.1644, -0.1315,  0.1501],\n",
      "          [-0.1954, -0.1090,  0.0033,  0.0997,  0.1922],\n",
      "          [ 0.1370,  0.0153,  0.1793,  0.0354,  0.0948]]]], requires_grad=True)\n",
      "torch.Size([3, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# layer의 weight 확인\n",
    "print(layer.weight)\n",
    "\n",
    "print(layer.weight.shape)\n",
    "# 결과의 3,1,5,5는 batch_size, channels, height, width의 크기를 가짐"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "d2d9d41be2eccbb64661f9358468e73863d139096b4b7eb16a65e205598038ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
